{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CJpJKaPtW1PU"
   },
   "source": [
    "#**Coronary Heart Disease Prediction**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "noWjjpTyo6aq"
   },
   "source": [
    "##**Data Description**\n",
    "###**Demographic:**\n",
    "###• Sex: male or female(\"M\" or \"F\")\n",
    "###• Age: Age of the patient;(Continuous - Although the recorded ages have been truncated to whole numbers, the concept of age is continuous)\n",
    "###**Behavioral:**\n",
    "###• is_smoking: whether or not the patient is a current smoker (\"YES\" or \"NO\")\n",
    "###• Cigs Per Day: the number of cigarettes that the person smoked on average in one day.(can be considered continuous as one can have any number of cigarettes, even half a cigarette.)\n",
    "###**Medical( history):**\n",
    "###• BP Meds: whether or not the patient was on blood pressure medication (Nominal)\n",
    "###• Prevalent Stroke: whether or not the patient had previously had a stroke (Nominal)\n",
    "###• Prevalent Hyp: whether or not the patient was hypertensive (Nominal)\n",
    "###• Diabetes: whether or not the patient had diabetes (Nominal)\n",
    "### **Medical(current):**\n",
    "###• Tot Chol: total cholesterol level (Continuous)\n",
    "###• Sys BP: systolic blood pressure (Continuous)\n",
    "###• Dia BP: diastolic blood pressure (Continuous)\n",
    "###• BMI: Body Mass Index (Continuous)\n",
    "###• Heart Rate: heart rate (Continuous - In medical research, variables such as heart rate though infact discrete, yet are considered continuous because of large number of possible values.)\n",
    "###• Glucose: glucose level (Continuous)\n",
    "\n",
    "###**Predict variable (desired target)**\n",
    "###• 10-year risk of coronary heart disease CHD(binary: “1”, means “Yes”, “0” means “No”) - DV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Mt1van-WniU6"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'math' from 'numpy' (C:\\Users\\rajee\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m math\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'math' from 'numpy' (C:\\Users\\rajee\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\__init__.py)"
     ]
    }
   ],
   "source": [
    "# Importing the libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import math\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1IchlHVxnONO",
    "outputId": "5ab6ab30-5993-4ebe-b0be-3f220d62704c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NnTPArtMn5FY"
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv('Cardiovascular Risk Prediction/data_cardiovascular_risk.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f-4M5N0fXfil",
    "outputId": "5f682ddb-d24f-4ed0-99a8-7c2ed09a3f0a"
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "id": "Zgl4FQUnoIct",
    "outputId": "9808d577-332d-40bc-d8ea-42c66b97461a"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w5do695kXLeB",
    "outputId": "dd4e8529-2011-4488-b5f9-0b3e974a71d8"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nzijwjQHXnXJ"
   },
   "source": [
    "###Here, we can observe that the dataframe contains 3390 rows and 17 variables with some missing values. Lets have a look into the data. As it contains some categorical and numerical variables, lets just have a look into the value counts of all categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jTgSn8AZXmvU",
    "outputId": "b669b147-2578-48b8-d154-584e2dba8c7e"
   },
   "outputs": [],
   "source": [
    "for i in df.columns:\n",
    "   print({i:df[i].nunique()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rU96pTJfX_Jt"
   },
   "source": [
    "###Now, we can see that how many categories and how many unique values are present in categorical variables and numerical variables. So for further study, we are defining two lists as categorical and numerical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "47wF57MzX-vV"
   },
   "outputs": [],
   "source": [
    "#defining numeric and categorical column to treat null values based on that\n",
    "categorical_columns = ['education','cigsPerDay','sex','is_smoking','BPMeds','prevalentStroke','prevalentHyp','diabetes','TenYearCHD']\n",
    "numerical_columns = ['age','totChol','sysBP','diaBP','BMI','heartRate','glucose']\n",
    "#Here age,CigsPerDay,totChol,sysBP,diaBP,heartrate and glucose are discrete numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ialhu8p-YOyf"
   },
   "outputs": [],
   "source": [
    "#defining categorical variables as categorical\n",
    "for i in categorical_columns:\n",
    "  df[i].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DmE2z-GQY3z-",
    "outputId": "70144668-f73e-463e-abd7-2e7597622e3d"
   },
   "outputs": [],
   "source": [
    "#checking the categories in each categorical column\n",
    "for i in categorical_columns:\n",
    "  if i!='cigsPerDay':\n",
    "    print(i,df[i].value_counts().reset_index(),\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eR8yEXVfaQYj"
   },
   "source": [
    "\n",
    "###Here TenYearCHD is the dependent variable and we can observe that only 16% (511/3390) of the total dataset is in '1' class, remaining 2879/3390 are in '0' class. We may face some difficulty in building model regarding this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8VEVh7-KagDs"
   },
   "source": [
    "##**Exploratory data Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pc6SJuNhaPnC",
    "outputId": "a10fc474-4636-48be-8c41-92b6cdcbb9ec"
   },
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b_WUcLbZbC-f"
   },
   "source": [
    "###**1. Visualizing Missing values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hp--OwEkbNVH",
    "outputId": "d798abfc-9794-4333-90cc-d9039250326c"
   },
   "outputs": [],
   "source": [
    "def missing_values_table(df):\n",
    "        mis_val =df.isna().sum()\n",
    "        mis_val_percent = 100 *df.isna().sum() / len(df)\n",
    "        mz_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "        mz_table = mz_table.rename(columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
    "        mz_table['Data Type'] = df.dtypes\n",
    "        mz_table = mz_table.sort_values('% of Total Values', ascending=False).round(1)\n",
    "        print (\" selected dataframe has \" + str(df.shape[1]) + \" columns and \" + str(df.shape[0]),\"\\n\\n\")\n",
    "#         mz_table.to_excel('D:/sampledata/missing_and_zero_values.xlsx', freeze_panes=(1,0), index = False)\n",
    "        return mz_table\n",
    "\n",
    "missing_values_table(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YYwI5ZgOblUw",
    "outputId": "e58fa18a-0e10-43a1-de1a-70e36c1a5178"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "sns.heatmap(df.isnull(),cbar=False,yticklabels=False,cmap='Greens');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ezG1qhmgbtTn"
   },
   "source": [
    "###Here, we can observe that 7 variables are having missing values. And 'glucose' is the variable with 9% of missing data. We will deal with the missing values once we are done with the EDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J2XxXAoMbzSN"
   },
   "source": [
    "##**Univariate Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jHcp4hYNb7g2"
   },
   "source": [
    "###**( a ) Analysing the data through pie chart**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JTFDTfk6bwPN",
    "outputId": "5a3dbfa7-5a32-4855-c52b-390ae554896b"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (30, 40))\n",
    "\n",
    "plt.subplot(3,2,1)\n",
    "labels = 'Non Risk',\"Risk\"\n",
    "plt.pie(df['TenYearCHD'].value_counts(), labels=labels ,autopct='%1.0f%%')\n",
    "plt.title(\"Cardiovascular Risk rate\")\n",
    "\n",
    "plt.subplot(3,2,2)\n",
    "labels = 'Not Under BP medication',\"under BP medication\"\n",
    "plt.pie(df['BPMeds'].value_counts(), labels=labels ,autopct='%1.0f%%')\n",
    "plt.title(\"Blood pressure rate of people\")\n",
    "\n",
    "plt.subplot(3,2,3)\n",
    "labels = 'No Stroke','Stroke'\n",
    "plt.pie(df['prevalentStroke'].value_counts(), labels=labels ,autopct='%1.0f%%')\n",
    "plt.title(\"% people who had Stroke previously\")\n",
    "\n",
    "plt.subplot(3,2,4)\n",
    "labels = 'Not hyper tensive','hypertensive'\n",
    "plt.pie(df['prevalentHyp'].value_counts(), labels=labels ,autopct='%1.0f%%')\n",
    "plt.title(\"% people who had hypertension previously\")\n",
    "\n",
    "plt.subplot(3,2,5)\n",
    "labels = 'No diabetis','having diabetis'\n",
    "plt.pie(df['diabetes'].value_counts(), labels=labels ,autopct='%1.0f%%')\n",
    "plt.title(\"% people who had diabetes \")\n",
    "\n",
    "plt.subplot(3,2,6)\n",
    "labels = 'education 1','education 2','education 3','education 4'\n",
    "plt.pie(df['education'].value_counts(), labels=labels ,autopct='%1.0f%%')\n",
    "plt.title(\"Education level of people \")\n",
    "\n",
    "plt.subplots_adjust(hspace= 0.8, wspace= 0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZLGAknQZcdH2"
   },
   "source": [
    "###Now, we can conclude that,\n",
    "\n",
    "(1) There are 85% of people are actually not at risk of Cardio Vascular Risk.\n",
    "\n",
    "(2) There are only 3% of people who are under BP medication.\n",
    "\n",
    "(3) There are only 1% of people who had stroke previously.\n",
    "\n",
    "(4) There are 32% of people who are having Hyper Tension.\n",
    "\n",
    "(5) There are 97% of the people who are non diabetic.\n",
    "\n",
    "(6) There are 11%(least) of the people are having highest level education and 42%(highest) of the people are having basic education level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gQMGUs2fcuek"
   },
   "source": [
    "###**Numerical data Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c4fDct60cnXQ",
    "outputId": "bf064917-c4fd-4655-ebb2-40d9e5470aa5"
   },
   "outputs": [],
   "source": [
    "sns.set(rc = {'figure.figsize':(15,8)})\n",
    "for i in numerical_columns:\n",
    "  if i != 'id':\n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.histplot(df[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QKqVRqHmdDgv"
   },
   "source": [
    "###Here we tried plotting Histogram for all numerical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FyyU72ukdJqE"
   },
   "source": [
    "##**Bivariate Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZEp-zswHdQT9"
   },
   "source": [
    "###**( a ) Numerical columns with the dependent variable**\n",
    "####**( i ) Violin plot for all numerical varibles along with dependent variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7NNoTKDgdGME",
    "outputId": "27f2d0f6-3ec8-4438-cc23-ba700c05d3b9"
   },
   "outputs": [],
   "source": [
    "numerical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QUBKMaTadlZF",
    "outputId": "985f9944-5615-42dd-90b6-90003539b329"
   },
   "outputs": [],
   "source": [
    "for i in numerical_columns:\n",
    "  sns.catplot(x=\"TenYearCHD\", y=i, kind='violin',data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9PDE2q9edzFT"
   },
   "source": [
    "###**( ii ) Bar plot for al the numerical variables along with dependent variable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3P_AdwSieJW3",
    "outputId": "4a8e7b7d-f020-448b-b6ad-180d8bf45f39"
   },
   "outputs": [],
   "source": [
    "numerical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xji5vcGzeMPo",
    "outputId": "b2f497b6-2d79-4abc-d434-d322f50c304b"
   },
   "outputs": [],
   "source": [
    "from matplotlib import rcParams\n",
    "\n",
    "def NumPlot(df, col):\n",
    "    # rcParams['figure.figsize'] = 11, 8\n",
    "    plt.figure(figsize=(16,8))\n",
    "    plt.xticks(rotation=90)\n",
    "    sns.countplot(x=col, hue='TenYearCHD', data = df, palette=\"Set2\")\n",
    "    plt.title('Comparation - {}'.format(col))\n",
    "    plt.legend(['CVR (─)', 'CVR (+)'])\n",
    "\n",
    "for i in numerical_columns:\n",
    "  NumPlot(df,i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ss4YB6jOeerD"
   },
   "source": [
    "###Here, we tried plotting all numerical varables with the dependent variables. From here we can conclude that,\n",
    "\n",
    "(1) The major people who are having Cardio Vasclar Risk(CVR) are at the age of 50-70.\n",
    "\n",
    "(2) The cholestrol level of people is same for both kind of people who are at risk of CVR and not at risk of CVR. Instead fewer people who are not at risk of CVR are having high Cholestrol level.\n",
    "\n",
    "(3) If we consider sysBP and diaBP together into consideration, then most of the people are having normal BP. So its hard to conclude to here about the CVR.\n",
    "\n",
    "(4) Even though many people are having normal range of BMI, but the people whoevever are having high BMI, they are at risk of CVR.\n",
    "\n",
    "(5) Many people are having normal heartrate range, so its not appropriate to come into conclusion about the CVR at this stage.\n",
    "\n",
    "(6) In glucose level, we can see some outliers in both kind of people(whoa are at risk and not at risk). But the people who got high glucose level are coming into the category of CVR. So we can conclude that its even one of the factor which may contribute to CVR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6tVV2RMOepo-"
   },
   "source": [
    "###**( b ) Categorical variable with the dependent variable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9iRR34DeuaY",
    "outputId": "d05d97a9-9700-4dad-e890-8b38b71630a7"
   },
   "outputs": [],
   "source": [
    "categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0P62APuGe3QN",
    "outputId": "e421684f-cd19-44e3-d219-4d58310f0fb1"
   },
   "outputs": [],
   "source": [
    "def habitPlot(df, col):\n",
    "    sns.countplot(x= col,\n",
    "                  hue= 'TenYearCHD',\n",
    "                  data= df, palette=\"Set2\")\n",
    "    plt.title('Comparation - {}'.format(col))\n",
    "    plt.legend(['CVR (─)', 'CVR (+)'])\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (20, 12))\n",
    "fig.suptitle('CVR -> Cardio Vascular Risk Disease')\n",
    "\n",
    "plt.subplot(4,2,1)\n",
    "habitPlot(df, 'education')\n",
    "\n",
    "plt.subplot(4,2,2)\n",
    "habitPlot(df, 'sex')\n",
    "\n",
    "plt.subplot(4,2,3)\n",
    "habitPlot(df, 'is_smoking')\n",
    "\n",
    "plt.subplot(4,2,4)\n",
    "habitPlot(df, 'BPMeds')\n",
    "\n",
    "plt.subplot(4,2,5)\n",
    "habitPlot(df, 'prevalentStroke')\n",
    "\n",
    "plt.subplot(4,2,6)\n",
    "habitPlot(df, 'prevalentHyp')\n",
    "\n",
    "plt.subplots_adjust(hspace= 0.6, wspace= 0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GYsUbBrpfE82"
   },
   "source": [
    "###Here, we are plotting the graph for the dependent variable along with the categorical variables present in the dataset.\n",
    "\n",
    "We can conclude from here that,\n",
    "\n",
    "(1) The people whoever already under the HyperTension, are at more risk of CVR\n",
    "\n",
    "(2) But its not the same in case of people who were under the attack of stroke once before. The people who never got the stroke are at high risk of CVR.\n",
    "\n",
    "(3) Whether people smoke or not smoke, they are at risk of CVR.\n",
    "\n",
    "(4) Its shocking to see that the people who never got any medication barely comes under the risk of CVR. The people who are under BP medications are at high risk of CVR.\n",
    "\n",
    "(5) When we compare males and females, males are at more risk of CVR.\n",
    "\n",
    "(6) We can clearly see that, the people who had only basic education i.e., education 1 are at more risk of CVR. And its gradually decreasing with increase in education. It might be because that the people wo are educated are taking much precaustions to avoid CVR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EtcfkXW4fJbl"
   },
   "source": [
    "##**Multivariate Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UHwKJBtEfHJk",
    "outputId": "277744be-17a3-4104-979b-ff2b102e3aae"
   },
   "outputs": [],
   "source": [
    "numerical_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5PcygFPcfTNz"
   },
   "source": [
    "###Here we are trying to get some conclusions about some numerical columns. So we tried plotting the same along with CVR and few more important variables whichever is useful to to come to conclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iBhjVUwYfdwB"
   },
   "source": [
    "###**( a ) Age and CVR with other numerical columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XC9hZltOfghD",
    "outputId": "e4b7d9e6-d88f-456e-8572-d1b528923c06"
   },
   "outputs": [],
   "source": [
    "for i in numerical_columns:\n",
    "  if i!='age':\n",
    "    sns.catplot(x=\"age\", y=i, hue=\"TenYearCHD\", kind=\"bar\", data=df,height=5, aspect=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EygNDfUAfrWE"
   },
   "source": [
    "###Here we can observe that,\n",
    "\n",
    "(1) The people who were under the risk of CVR were from the age>34. And age did not matter to any of other numerical variables. We can see same level of measures such as BP, BMI etc., for all age group.\n",
    "\n",
    "(2) The cholestrol level for these peple is slighly more when we compare it with the people who are not at risk of CVR. And at the age of 70, even though they were having slighly low level of cholestrol, they were at risk of CVR.\n",
    "\n",
    "(3) If we consider sysBP, diaBP, heart rate and BMI together for the overall conclusion, we can conclude that all the people who are at risk are having high values of these measures than the people who are not at risk of CVR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Ix9Asr0fxGf"
   },
   "source": [
    "###**( b ) Education and CVR with other numerical columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-75DKUFBfYpf",
    "outputId": "cf75aba9-45b0-46dc-9a44-6ff20b5e0b90"
   },
   "outputs": [],
   "source": [
    "for i in numerical_columns:\n",
    "  sns.catplot(x=\"education\", y=i, hue=\"TenYearCHD\", kind=\"violin\", data=df,height=5, aspect=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_O8Up1kgEi2"
   },
   "source": [
    "###From this we can conclude that,\n",
    "\n",
    "(1) The people who had the basic level education are at more risk of CVR when we compare the levels of education.\n",
    "\n",
    "(2) Cholestrol level is high for fewer people in education level 2. But people who had basic education were at more risk of CVR.\n",
    "\n",
    "(3) People with only the basic education are having more BP(considering sysBP,diaBP together), heartrate and BMI as well. So they are directly at more risk of CVR.\n",
    "\n",
    "(4) People with the highest education (Education 4) are having controlled balanced glucose level. But other fewer people with other education levels are having very high cholestrol. We cna see a peak in glucose level in education leval 3 group peopl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uu3mDp7JgNXl"
   },
   "source": [
    "###**( c ) Sex and CVR with other numerical columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W9yjKJDbgILI",
    "outputId": "ae94f49d-b9f0-4c20-944d-53b992e77e72"
   },
   "outputs": [],
   "source": [
    "for i in numerical_columns:\n",
    "  sns.catplot(x=\"sex\", y=i, hue=\"TenYearCHD\", kind=\"violin\", data=df,height=5, aspect=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t2uV1SsWgnPf"
   },
   "source": [
    "###From here we can conclude that,\n",
    "\n",
    "(1) Females are having high BP, high heart rate, high BMI and even high values of glucose. But the females who are between the age group of 50-70 are at more risk of CVR.\n",
    "\n",
    "(2) Males between the age group 40-70 are at more risk of CVR.\n",
    "\n",
    "(3)Whoever had the highest glucose level amongst men all comes under the risk of CVR.\n",
    "\n",
    "(4) We can see that some of the highest cholestrol values are obtained by men (alone). This might be the reason that they are at high risk of CVR.\n",
    "\n",
    "(5) But many men maintained normal range of BP, heartrate and BMI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Glzpylqrh4L0"
   },
   "source": [
    "###**( d ) Diabetes and CVR with other numerical columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B3Gvf5hzgpSX",
    "outputId": "ecc37c9d-b5dd-4e09-e4e1-8879e1f5bcce"
   },
   "outputs": [],
   "source": [
    "for i in numerical_columns:\n",
    "  sns.catplot(x=\"diabetes\", y=i, hue=\"TenYearCHD\", kind=\"violin\", data=df,height=5, aspect=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g-dq90FqiK9e"
   },
   "source": [
    "###Here, we can observe that,\n",
    "\n",
    "(1) Even though they had diabetes or not, they are at same level of risk of CVR. But its high between the age group 50-70.\n",
    "\n",
    "(2) We say that cholestrol and diabetes are related, but from the data, its contrary to our assumption. Even though many people have very high level of high cholestrol, they were not at risk of diabetes nor CVR. But if people had diabetes and cholestrol levels are high then they are at high risk of CVR.\n",
    "\n",
    "(3) BP and heartrate has nothing to do th the diabetes in here. Many people who are at risk of CVR and diabetes are actually maintaining a normal range of BP and heart rate.\n",
    "\n",
    "(4) If people are already having diabetes and if they have high BMI then they are at high risk of CVR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OM6mdol8iPY7"
   },
   "source": [
    "###**( e ) Some more analysis about Sex column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "un40a-sciM9u",
    "outputId": "69c6b6ca-5bd6-4121-d478-ba5a132fa94c"
   },
   "outputs": [],
   "source": [
    "sns.catplot(x=\"education\", y=\"glucose\", hue=\"TenYearCHD\",\n",
    "            col=\"sex\", aspect=.7,\n",
    "            kind=\"bar\", data=df)\n",
    "#you can change kind to bar/swan/anything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zxBwk7SoikB4"
   },
   "source": [
    "###From here we can conclue that,\n",
    "\n",
    "(1) Women with the highest education level are maintaining normal level of glucose. And slightly at low risk of CVR when compared to men with the highest education.\n",
    "\n",
    "(2) Women with the education 2 are having high level of glucose and men with education 3 are having highest level of glucose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e8p7QPoPiorB",
    "outputId": "39c920e3-94e7-41ed-da69-ff8084cada93"
   },
   "outputs": [],
   "source": [
    "sns.catplot(x=\"education\", y=\"is_smoking\", hue=\"TenYearCHD\",\n",
    "            col=\"sex\", aspect=.7,\n",
    "            kind=\"violin\", data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DLonf_D7iyLj"
   },
   "source": [
    "###We can clearly see that people with the basic level education are smoking alot. and they are at highest risk of CVR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WunTCM99i2Vu",
    "outputId": "6d415196-3699-409c-ee49-e3ad6726aa0e"
   },
   "outputs": [],
   "source": [
    "sns.catplot(x=\"age\", y=\"glucose\", hue=\"TenYearCHD\",\n",
    "            col=\"sex\", aspect=1.8,\n",
    "            kind=\"bar\", data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q0FxFvRojJCQ",
    "outputId": "36a65c2f-5f0b-4088-c6d0-584515fef718"
   },
   "outputs": [],
   "source": [
    "sns.catplot(x=\"age\", y=\"totChol\", hue=\"TenYearCHD\",\n",
    "            col=\"sex\", aspect=1.8,\n",
    "            kind=\"bar\", data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d3VuFgubjWjZ"
   },
   "source": [
    "###Here we can see that, People of age (40-70) whoever are having high level of glucose and cholestrol, all are at high rik of CVR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5xqKkDeRjdHv"
   },
   "source": [
    "##**Distribution plot of all numerical variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ctDrrptmjYNw",
    "outputId": "e7e43ecb-50ee-41a8-b2b0-1a26e31f7d19"
   },
   "outputs": [],
   "source": [
    "#Min max scaler\n",
    "column_names = numerical_columns\n",
    "# column_names\n",
    "#taking columns to do the minmaxscaling\n",
    "cardio_2 = pd.DataFrame()\n",
    "#using standardization as both numeric columns are in different scale\n",
    "scaler = MinMaxScaler()\n",
    "scaled = scaler.fit_transform(df[numerical_columns])\n",
    "#print(scaled)\n",
    "cardio_2 = pd.DataFrame(scaler.fit_transform(df[numerical_columns]))\n",
    "cardio_2.columns = column_names\n",
    "\n",
    "sns.set(rc = {'figure.figsize':(15,8)})\n",
    "cardio_2.plot.kde()\n",
    "plt.title(\"Distribution plot\")\n",
    "plt.xlabel(\"Numerical variables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OOVv5G4jj21z"
   },
   "source": [
    "###We can observe that there are many people with high level of glucose followed by cholestrol. It might be that the lifestyle of people are contributing more to these values. So we observe some peaks in these vlaues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3c3Qopynj-IZ"
   },
   "source": [
    "#**Treating Missing values**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vUqzPSbAkGkb"
   },
   "source": [
    "###We had already seen that there are 6 variables with missing values. Now we try treating all of the variables one by one. First lets start from categorical variables. We try to replace the missing values by mode of the categorical varibles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ouTtMitkL_k"
   },
   "source": [
    "###**( a ) Education null values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prexlNfPj4qB"
   },
   "outputs": [],
   "source": [
    "#Education null values\n",
    "df['education'].fillna(df['education'].median(),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g5ZuKFzyknmq",
    "outputId": "90712485-7b5f-4a93-e07a-28dc54ec87bf"
   },
   "outputs": [],
   "source": [
    "df['education'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RS0V9gYSlA-A"
   },
   "source": [
    "###**( b ) BPMeds null values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WO0h7V-6lFko",
    "outputId": "9250bda1-ce61-4047-cb12-ceaa366432ab"
   },
   "outputs": [],
   "source": [
    "#BPMeds\n",
    "df['BPMeds'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fCprP1PXlN4V"
   },
   "outputs": [],
   "source": [
    "#Treating null values\n",
    "df['BPMeds'] = df['BPMeds'].fillna(df['BPMeds'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9tB4tbXqla5J",
    "outputId": "eeb9da8a-88ed-4fb6-ee92-76c9d23c36bd"
   },
   "outputs": [],
   "source": [
    "df['BPMeds'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LcxFHAN4l6VC"
   },
   "source": [
    "###**( c ) cigsPerDay null values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9uNV6ldAl8wk",
    "outputId": "6142ad5a-084b-4afd-af23-fe4f93f42b06"
   },
   "outputs": [],
   "source": [
    "#cigsPerDay null values\n",
    "sns.histplot(df['cigsPerDay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rQOCDYxrmEv6",
    "outputId": "ab73ec0a-5b96-4e39-a76c-3abcc7fad423"
   },
   "outputs": [],
   "source": [
    "df['cigsPerDay'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zCU7cdB8mJ3v"
   },
   "outputs": [],
   "source": [
    "#as we can see even though there are 32 different ans for cigsPer day..we have a positively skewed data\n",
    "#so we can create nother category over here.\n",
    "\n",
    "df['cigsPerDay'].astype(\"category\")\n",
    "\n",
    "cigsPerDay_data = df['cigsPerDay'].value_counts().reset_index()\n",
    "cigsPerDay_data.rename(columns={'index':'Number_of_cigar','cigsPerDay':'Number_of_people'},inplace=True)\n",
    "cigsPerDay_data.sort_values('Number_of_cigar',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B3K-LOlGmS-Q",
    "outputId": "9fdeb944-833e-4ae9-d9e0-c28601eee766"
   },
   "outputs": [],
   "source": [
    "df['cigsPerDay'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dFj2YAEBmYMZ"
   },
   "outputs": [],
   "source": [
    "df['cigsPerDay'] = df['cigsPerDay'].fillna(df['cigsPerDay'].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zzP-wFrFmi_h"
   },
   "source": [
    "###Here,\n",
    " we handled the cigsPerDay variable like a categorical variable, its because there are very few unique values present in this column. We completely convert this column as categorical column later while doing the feature engineering.\n",
    "\n",
    "Now that all the categorical variables missing values are treated with mode, we now try to handle the missing values of Numerical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tQxZRPoOmyFD"
   },
   "source": [
    "###**( d ) numeric variables null calues**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bQSLzZS1muhx",
    "outputId": "df427544-022f-472b-d7d7-791830913cc9"
   },
   "outputs": [],
   "source": [
    "#Now we need to treat Missing values with only numeric variables\n",
    "numeric_NA=[]\n",
    "for i in numerical_columns:\n",
    "  if df[i].isna().sum()>0:\n",
    "    numeric_NA.append(i)\n",
    "for i in numeric_NA:\n",
    "  plt.figure(figsize=(8,6))\n",
    "  print(i,' Null values are :',round(df[i].isna().sum()/len(df)*100,4))\n",
    "  sns.distplot(df[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sXcSyHK0nHzI"
   },
   "outputs": [],
   "source": [
    "#Here we can observe that all the variables are normally distributed\n",
    "# and the number of missing values are very less for totChol,BMI and heartRate (<2%)\n",
    "#But for glucose MV % is ~9%\n",
    "# so we equate MV of first 3 with mean\n",
    "# and we use KNN imputer to find the MV of glucose\n",
    "def fillna_numeric_with_mean(df,col):\n",
    "  df[col] = df[col].fillna(df[col].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UglWUwO2nMOf"
   },
   "outputs": [],
   "source": [
    "for i in numeric_NA:\n",
    "  if i!= 'glucose':\n",
    "    fillna_numeric_with_mean(df,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hP8YjSYInSjQ"
   },
   "outputs": [],
   "source": [
    "#KNN to find the missing values for glucose\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Defining scaler and imputer objects\n",
    "scaler = StandardScaler()\n",
    "imputer = KNNImputer()\n",
    "\n",
    "# Imputing missing values with KNN if any\n",
    "df['glucose'] = imputer.fit_transform((df['glucose'].values.reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GZveMrbvnZRk",
    "outputId": "351ad558-f837-4134-fb71-da9f0236fd53"
   },
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GJGatYW4nfVy"
   },
   "source": [
    "###Now that all the missing values are treated and even we can see that the distributions were not aletered by handling the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GEoyjnJ7nhHf",
    "outputId": "91361c26-0536-45a9-e31a-c42cf0b087b5"
   },
   "outputs": [],
   "source": [
    "for i in numeric_NA:\n",
    "  plt.figure(figsize=(8,5))\n",
    "  sns.distplot(df[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HrUN7Nbvntj3"
   },
   "source": [
    "##**Treating Outliers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uh7o1egTnwsB",
    "outputId": "b1358209-75c6-45aa-8471-91c9f8c501bd"
   },
   "outputs": [],
   "source": [
    "df[numerical_columns].describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UBvlYuFRoBpK"
   },
   "outputs": [],
   "source": [
    "df.drop('id',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "STGV6t-OoF9Y",
    "outputId": "5ee52770-5cac-4e28-9192-2c28749ac4b1"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S72FpLf6oKUj"
   },
   "source": [
    "###There are not many outliers present in the data. But outliers should never be neglected, so we try treating the outliers by Z-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kElTYomxoOvr"
   },
   "outputs": [],
   "source": [
    "#Z score treatment\n",
    "def remove_outlier(df,column):\n",
    "\n",
    "  plt.figure(figsize=(15,6))\n",
    "  plt.subplot(1, 2, 1)\n",
    "  plt.title('Before Treating outliers')\n",
    "  sns.boxplot(df[column])\n",
    "  plt.subplot(1, 2, 2)\n",
    "  sns.distplot(df[column])\n",
    "  df = df[((df[column] - df[column].mean()) / df[column].std()).abs() < 3]\n",
    "  df = df[((df[column] - df[column].mean()) / df[column].std()).abs() > -3]\n",
    "\n",
    "  plt.figure(figsize=(15,6))\n",
    "\n",
    "  plt.subplot(1, 2, 1)\n",
    "  plt.title('After Treating outliers')\n",
    "  sns.boxplot(df[column])\n",
    "  plt.subplot(1, 2, 2)\n",
    "  sns.distplot(df[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pmpzy8Y3oWT5",
    "outputId": "26fa3fee-0ce2-49b9-9b64-530e7a2b9e97"
   },
   "outputs": [],
   "source": [
    "for column in numerical_columns:\n",
    "  remove_outlier(df,column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "88Jp6DXooiP6"
   },
   "outputs": [],
   "source": [
    "df_2 =df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tayxOGxjopOm"
   },
   "source": [
    "##**Feature Engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SQ_LW56Sosfa",
    "outputId": "dfae1d16-8d55-4342-ee70-bba13a2970b8"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xbRjU_Mboz2i"
   },
   "source": [
    "###**1. Converting cigsPerDay into a categorical column**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "twFPj0HBo6_B"
   },
   "source": [
    "###As we already mentioned, cigsPerDay is the column with very less unique values. It was a discrete numerical variable. So we convert this into a categorical column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m7Sxq61co9WF",
    "outputId": "1ebf0007-cc89-497d-80c8-3e1447b2e205"
   },
   "outputs": [],
   "source": [
    "# herewe can observe that the distribution is not equal\n",
    "# so we can create another category that\n",
    "# with 0 cigars = 1st category\n",
    "# between 1-20 = 2nd category\n",
    "# between 20-70 = 3rd category\n",
    "for i in range(len(df)):\n",
    "  if df['cigsPerDay'][i] == 0:\n",
    "    df['cigsPerDay'][i] = 'No Cunsumption'\n",
    "  elif df['cigsPerDay'][i] > 0 and df['cigsPerDay'][i] < 20:\n",
    "    df['cigsPerDay'][i] = 'Average consumtion'\n",
    "  else:\n",
    "    df['cigsPerDay'][i] = 'High Consumption'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VY01me8to2qZ",
    "outputId": "fd44a8ed-5f48-4665-81d6-a5c2798ede26"
   },
   "outputs": [],
   "source": [
    "df['cigsPerDay'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u27hvlIcpZeA"
   },
   "source": [
    "###**2. Creating a new variable from sysBP and diaBP**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VhAJ5ap2peyQ"
   },
   "source": [
    "###Whenever we analyse the BP, we consider sysBP and diaBP together to ge the better result. So here also, we consider these vaues together and create a new categorical column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vWiMhFDnpccj"
   },
   "outputs": [],
   "source": [
    "df['BP'] = 0\n",
    "\n",
    "df.loc[(df['sysBP'] < 120) & (df['diaBP'] < 80), 'BP'] = 1\n",
    "\n",
    "df.loc[((df['sysBP'] >= 120) & (df['sysBP'] < 130)) &\n",
    "         ((df['diaBP'] < 80)), 'BP'] = 2\n",
    "\n",
    "df.loc[((df['sysBP'] >= 130) & (df['sysBP'] < 140)) |\n",
    "         ((df['diaBP'] >= 80) & (df['diaBP'] < 90)), 'BP'] = 3\n",
    "\n",
    "df.loc[((df['sysBP'] >= 140) & (df['sysBP'] < 180)) |\n",
    "         ((df['diaBP'] >= 90) & (df['diaBP'] < 120)), 'BP'] = 4\n",
    "\n",
    "df.loc[(df['sysBP'] >= 180) | (df['diaBP'] >= 120), 'BP'] = 5\n",
    "\n",
    "cols_BP = ['sysBP', 'diaBP']\n",
    "df.drop(cols_BP, axis= 1, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-ITauH1EqA-Z",
    "outputId": "24fc1ac5-1b8a-4a72-e16d-e92a45812089"
   },
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "32SOCoKCqGI6"
   },
   "source": [
    "###**3. Checking Multicollinearity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4WVqDBSvqKtC",
    "outputId": "3b5f0c55-abf4-472a-cb47-40e0a787460a"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(round(df.corr(),2),annot=True,cmap='Greens');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vu8ti70lqSjs"
   },
   "outputs": [],
   "source": [
    "#Drop DiaBP as its highly correlated to SysBP, prevalentHyp and diabetes\n",
    "#df.drop('diaBP',axis=1,inplace=True)\n",
    "df.drop('prevalentHyp',axis=1,inplace=True)\n",
    "df.drop('diabetes',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qd_8HnSXqZ1B",
    "outputId": "d1cde6bb-198a-4186-9348-ca0976f66284"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(round(df.corr(),2),annot=True,cmap='Greens');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6i7VcOEhqnPx"
   },
   "source": [
    "###Multicollinearity should be checked till we build models to make sure that we are not adding any variables with high correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q8_SRluFqvag"
   },
   "source": [
    "###**4. Transformation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6IMm7G5Lqyl2"
   },
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "import pylab\n",
    "\n",
    "def to_plot(DF,column):\n",
    "  plt.figure(figsize=(10,6))\n",
    "  plt.subplot(1,2,1)\n",
    "  sns.distplot(DF[column])\n",
    "  plt.subplot(1,2,2)\n",
    "  stats.probplot(DF[column],dist='norm',plot=pylab)\n",
    "  plt.show()\n",
    "\n",
    "def log_transform(DF,column):\n",
    "  print(\"Before Transformation\")\n",
    "  to_plot(DF,column)\n",
    "  # applying log transformation\n",
    "  DF[column]=np.log1p(DF[column])\n",
    "  #plotting\n",
    "  print(\"After Transformation\")\n",
    "  to_plot(DF,column)\n",
    "  # stats.probplot()\n",
    "\n",
    "def box_cox_transform(DF,column):\n",
    "  print(\"Before Transformation\")\n",
    "  to_plot(DF,column)\n",
    "  # applying boxcox transformation\n",
    "  DF[column],parameters=stats.boxcox(DF[column])\n",
    "  print(\"After Transformation\")\n",
    "  to_plot(DF,column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l8lzd6-GqsCg"
   },
   "outputs": [],
   "source": [
    "numerical_columns = ['age', 'totChol', 'BMI', 'heartRate', 'glucose']\n",
    "#updating as we deleted sysBP and diaBP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z8t45bL8q-JW",
    "outputId": "ab9dd89e-0ce6-41d7-c038-fcc651d70fd4"
   },
   "outputs": [],
   "source": [
    "#Applying the transformaation to only numerical columns\n",
    "log_transform(df,'age')\n",
    "log_transform(df,'totChol')\n",
    "box_cox_transform(df,'BMI')\n",
    "log_transform(df,'heartRate')\n",
    "box_cox_transform(df,'glucose')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dpR-5W8CrUxJ"
   },
   "outputs": [],
   "source": [
    "df_3 = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aRz7Z6-fraZN"
   },
   "source": [
    "###Transformation is not a must thing to do in classification models. but taking a normal distribution variables always improve the performance of the model. So now, we are done with the transformatin, we will use the one-hot encoding to categorical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l_Q1vXZhrf_M"
   },
   "source": [
    "###**5. One hot encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NJHYIn-Brdhk"
   },
   "outputs": [],
   "source": [
    "categorical_columns = ['education','cigsPerDay', 'sex', 'is_smoking', 'BPMeds', 'prevalentStroke','TenYearCHD','BP']\n",
    "#Updating the categorical columns by removing prevaleHyp and diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3IlyAYgzro38",
    "outputId": "73cfc176-e6db-45b0-db75-a126baf1a182"
   },
   "outputs": [],
   "source": [
    "DF =df[categorical_columns]\n",
    "#dropping the dependent variable\n",
    "DF.drop('TenYearCHD',axis=1,inplace=True)\n",
    "DF = pd.get_dummies(DF, columns=DF.columns)\n",
    "DF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EzBul2cvsAgo"
   },
   "source": [
    "###So now we are done with the one-hot encoding, before building our model, we will apply standardization technique to our data to have a smae scale for all the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "noE0W26isIav"
   },
   "source": [
    "###**6. MinMaxScaler**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zYKfFovQsC_x",
    "outputId": "3ff1cd04-9502-4564-a64e-7ee79cd14c62"
   },
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-AsESuaWsX9B"
   },
   "outputs": [],
   "source": [
    "numerical_columns = ['age', 'totChol', 'BMI', 'heartRate', 'glucose']\n",
    "#Updated the numerical columns as we deleted 'diaBP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ph_sX6casbUS"
   },
   "outputs": [],
   "source": [
    "#Merginf categorical and numerical dependent variables\n",
    "DF_new = df[numerical_columns].copy()\n",
    "for i in DF.columns:\n",
    "  DF_new[i] = DF[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vpMSI419sgtR",
    "outputId": "1d9624ee-9489-470d-ad4d-9e6d3a7fcfb5"
   },
   "outputs": [],
   "source": [
    "DF_new.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8aiOdHxAs85j"
   },
   "outputs": [],
   "source": [
    "#Min max scaler\n",
    "column_names = list(DF_new.columns)\n",
    "\n",
    "# column_names\n",
    "\n",
    "#taking columns to do the minmaxscaling\n",
    "DF_scaled = pd.DataFrame()\n",
    "\n",
    "\n",
    "#using standardization as both numeric columns are in different scale\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaled = scaler.fit_transform(DF_new[DF_new.columns])\n",
    "#print(scaled)\n",
    "DF_scaled = pd.DataFrame(scaler.fit_transform(DF_new[DF_new.columns]))\n",
    "DF_scaled.columns = column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gRRxoSe3tBwi",
    "outputId": "c88a9c5d-9213-4765-e18e-c1d57f95bc9c"
   },
   "outputs": [],
   "source": [
    "DF_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K6P3QGuytGEL"
   },
   "source": [
    "###**7. Checking mulicollinearity again after one-hot encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sDVQgOPHtLJ8"
   },
   "outputs": [],
   "source": [
    "#We tried removing multicolliniearity with only the numerical columns\n",
    "#now that dummy variables are added, check again for collinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zHHxqV1ltO7D",
    "outputId": "a5f50e32-575b-4995-d8af-95355641f394"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,6))\n",
    "sns.heatmap(round(DF_scaled.corr(),2),annot=True,cmap='Greens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S7_I5IXRtWm8"
   },
   "outputs": [],
   "source": [
    "#We can drop is_smoking column as its highly correlated to consumption of cigarate\n",
    "DF_scaled.drop('is_smoking_YES',axis=1,inplace=True)\n",
    "DF_scaled.drop('is_smoking_NO',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t2RWNpBltaq5",
    "outputId": "824d7dc9-95c7-41a4-c850-9d5b77423c54"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,6))\n",
    "sns.heatmap(round(DF_scaled.corr(),2),annot=True,cmap='Greens');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1qDaVXmstirB"
   },
   "outputs": [],
   "source": [
    "#Whenever we have only car=tegories then we can drop one as we can its highly negatively correlated\n",
    "#those columns are sex,BPMeds,prevalenStroke\n",
    "#So we delete any one category from these\n",
    "DF_scaled.drop('sex_F',axis=1,inplace=True)\n",
    "DF_scaled.drop('BPMeds_0.0',axis=1,inplace=True)\n",
    "DF_scaled.drop('prevalentStroke_0',axis=1,inplace=True)\n",
    "DF_scaled.drop('cigsPerDay_No Cunsumption',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kCtUyRU2tmdM",
    "outputId": "aea4bfd4-78e1-4dc7-8e59-588521a0dabc"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,6))\n",
    "sns.heatmap(round(DF_scaled.corr(),2),annot=True,cmap='Greens');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dW1vmz7JtraL"
   },
   "source": [
    "###Now, our data is almost set to build a model. We created some variables, had performed one-hot encoding, standarization technique and even checked multicollinearity, we now need to do the feature selection from the set of variables we have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NPgS_t2Ptyrj"
   },
   "source": [
    "###**8. Feature selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nt-lYtMat3Vp"
   },
   "outputs": [],
   "source": [
    "#importing the libraries\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sc5825RgtvRZ",
    "outputId": "22c1f5e5-6d2e-4431-d7b1-59983ed18d5e"
   },
   "outputs": [],
   "source": [
    "DF_scaled.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "juCvJeXRt-Ky"
   },
   "outputs": [],
   "source": [
    "X = DF_scaled.copy()\n",
    "y = df['TenYearCHD'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n29l54RguDEq",
    "outputId": "1ca1f6a8-bf23-4c34-92f8-2d614d969438"
   },
   "outputs": [],
   "source": [
    "#finding the f scores of each features\n",
    "f_scores = f_regression(X, y)\n",
    "f_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AnvIFzKDuG_Y",
    "outputId": "fb0d2b55-222b-4eac-bb91-18049561bb37"
   },
   "outputs": [],
   "source": [
    "#The second array consists of p-values that we need. let's plot it\n",
    "p_values= pd.Series(f_scores[1],index= X.columns)\n",
    "p_values.plot(kind='bar',color='blue',figsize=(16,5))\n",
    "plt.title('P-value scores for numerical features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l2M2d9u_uKvq",
    "outputId": "350987a1-d3e4-48cb-c2c1-a92cf8713abb"
   },
   "outputs": [],
   "source": [
    "#taking the features whcih ever is having less f scores.\n",
    "selected_features = ['age','totChol','BMI','glucose','education_1.0','education_2.0','cigsPerDay_High Consumption','sex_M','BPMeds_1.0','prevalentStroke_1','BP_1','BP_3','BP_4','BP_5']\n",
    "len(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mnmBRV0guNwi",
    "outputId": "82902cad-0ed6-4376-a5cb-4bb5759f8abe"
   },
   "outputs": [],
   "source": [
    "# Lets look at the correlation matrix now.\n",
    "fig = plt.figure(figsize=(16,8))\n",
    "ax = fig.add_subplot(111)\n",
    "sns.heatmap(X[selected_features].corr(),annot=True, cmap='Greens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rm06tZX3uTnv"
   },
   "source": [
    "#**Train Test Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jgnfLHI9uWJ8"
   },
   "outputs": [],
   "source": [
    "X = X[selected_features]\n",
    "y = df['TenYearCHD'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wj-HPIE7ucAY"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test= train_test_split(X, y,  test_size= 0.30, random_state= 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "91m_k9UGufSJ"
   },
   "source": [
    "###**1. Treat Class imbalance by SMOTE or MSMOTE**\n",
    "As we had many categorical variables in the dataset, there arises a problem of class imbalance. Its very important to treat the class imbalance before building a model. So we try treating the class imbalance. Here we have used the method of SMOTE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AtCFrb8i9bhH",
    "outputId": "efcc5d33-d121-419c-d142-dab201374fc8"
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from collections import Counter\n",
    "\n",
    "# the numbers before SMOTE\n",
    "num_before = dict(Counter(y_train))\n",
    "\n",
    "#perform SMOTE\n",
    "\n",
    "# define pipeline\n",
    "over = SMOTE(sampling_strategy=0.8)\n",
    "under = RandomUnderSampler(sampling_strategy=0.8)\n",
    "steps = [('o', over), ('u', under)]\n",
    "pipeline = Pipeline(steps=steps)\n",
    "\n",
    "# transform the dataset\n",
    "X_smote, y_smote = pipeline.fit_resample(X_train, y_train)\n",
    "\n",
    "\n",
    "#the numbers after SMOTE\n",
    "num_after =dict(Counter(y_smote))\n",
    "print(num_before, num_after)\n",
    "\n",
    "#using class_wieghts\n",
    "\n",
    "class_weight = {0: 1,\n",
    "                1: 6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aM7O8hXT9bhJ",
    "outputId": "0cb3b418-5d50-447c-d146-c97ff9f111ce"
   },
   "outputs": [],
   "source": [
    "labels = [\"Negative Cases\",\"Positive Cases\"]\n",
    "plt.figure(figsize=(15,6))\n",
    "plt.subplot(1,2,1)\n",
    "sns.barplot(labels, list(num_before.values()))\n",
    "plt.title(\"Numbers Before Balancing\")\n",
    "plt.subplot(1,2,2)\n",
    "sns.barplot(labels, list(num_after.values()))\n",
    "plt.title(\"Numbers After Balancing\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aPaEOYCTuxeI"
   },
   "source": [
    "#**Build models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ofk81__su8dq"
   },
   "source": [
    "Now we start building models for our classiffication problem. We have used some of the models like,\n",
    "\n",
    "(1)Logistic regression\n",
    "\n",
    "(2)Support Vector Machine (SVM)\n",
    "\n",
    "(3)K nearest neighbour (KNN)\n",
    "\n",
    "(4)Naive Bayes\n",
    "\n",
    "(5)Decision Tree\n",
    "\n",
    "(6)Adaboost\n",
    "\n",
    "(7)Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jp--_GUFvA_p"
   },
   "source": [
    "##**1. Building all models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IBylHZkfu5aB"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV,RandomizedSearchCV,train_test_split\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,f1_score,roc_curve, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cpgsz5PIvJY_"
   },
   "outputs": [],
   "source": [
    "dtc = DecisionTreeClassifier()\n",
    "ran = RandomForestClassifier(n_estimators=90)\n",
    "knn = KNeighborsClassifier(n_neighbors=79)\n",
    "svm = SVC(random_state=6)\n",
    "lgr = LogisticRegression(solver='liblinear')\n",
    "adb = AdaBoostClassifier(algorithm='SAMME.R',random_state=42)\n",
    "nb = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Sz3xSYbvO_0"
   },
   "outputs": [],
   "source": [
    "models = {\"Decision tree\" : dtc,\n",
    "          \"Random forest\" : ran,\n",
    "          \"KNN\" : knn,\n",
    "          \"SVM\" : svm,\n",
    "          \"Logistic Regression\" : lgr,\n",
    "          \"Adaboost\" : adb,\n",
    "          \"Naive Bayes\" : nb}\n",
    "scores= { }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O1bQu2uhvRz5"
   },
   "outputs": [],
   "source": [
    "for key, value in models.items():\n",
    "    model = value\n",
    "    model.fit(X_train, y_train)\n",
    "    scores[key] = model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mh8drnP5vVUO",
    "outputId": "eddcf857-108c-4c83-8011-0e1b5dff33ac"
   },
   "outputs": [],
   "source": [
    "# after feature election\n",
    "scores_frame = pd.DataFrame(scores, index=[\"Accuracy Score\"]).T\n",
    "scores_frame.sort_values(by=[\"Accuracy Score\"], axis=0 ,ascending=False, inplace=True)\n",
    "scores_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0kfOsvCLvYzV"
   },
   "source": [
    "\n",
    "We can observe that the accuracy score is high in Adaboost. But by looking into just the accuracy we can not come to any conclusion. So we try the cross validation here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kkp7K0bDvp0l"
   },
   "source": [
    "##**2. Cross validation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xVUnODmCvxr1"
   },
   "source": [
    "###**i. ROC curve**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xk6lLfDuvk5O",
    "outputId": "51a3da76-21e2-48f8-a176-a0b88b18812f"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_roc_curve,classification_report\n",
    "\n",
    "disp = plot_roc_curve(dtc, X_test, y_test)\n",
    "plt.rcParams['figure.figsize'] = (10, 10)\n",
    "plot_roc_curve(ran,X_test, y_test, ax = disp.ax_)\n",
    "plot_roc_curve(knn,X_test, y_test, ax = disp.ax_)\n",
    "plot_roc_curve(svm,X_test, y_test, ax = disp.ax_)\n",
    "plot_roc_curve(lgr,X_test, y_test, ax = disp.ax_)\n",
    "plot_roc_curve(adb,X_test, y_test, ax = disp.ax_)\n",
    "plot_roc_curve(nb,X_test, y_test, ax = disp.ax_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ofC_9koiv-o5"
   },
   "source": [
    "If we have a look at ROC curve then we can conclude that Naive Bayes and logistic regression are performing well. But again this alone is not sufficient. So we have a look at the evaluation metric and confusion metric of these models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ubxSAMsJwBiC"
   },
   "source": [
    "###**ii. Confusion Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KXa3Bwecvacz",
    "outputId": "0c2c0825-74f8-4ace-cded-5cc743882bc3"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "classifiers = {\"Decision tree\" : dtc,\n",
    "          \"Random forest\" : ran,\n",
    "          \"KNN\" : knn,\n",
    "          \"SVM\" : svm,\n",
    "          \"Logistic Regression\" : lgr,\n",
    "          \"Adaboost\" : adb,\n",
    "          \"Naive Bayes\" : nb}\n",
    "\n",
    "f, axes = plt.subplots(1, 7, figsize=(20, 5), sharey='row')\n",
    "# fig, (ax1,ax2, ax3) = plt.subplots(1,7,nrows=3, sharex=True)\n",
    "for i, (key, classifier) in enumerate(classifiers.items()):\n",
    "    y_pred = classifier.fit(X_train, y_train).predict(X_test)\n",
    "    cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(cf_matrix,\n",
    "                                  display_labels=['No','Yes'])\n",
    "    disp.plot(ax=axes[i], xticks_rotation=45)\n",
    "    disp.ax_.set_title(key)\n",
    "    disp.im_.colorbar.remove()\n",
    "    disp.ax_.set_xlabel('')\n",
    "    if i!=0:\n",
    "        disp.ax_.set_ylabel('')\n",
    "\n",
    "f.text(0.4, 0.1, 'Predicted label', ha='left')\n",
    "\n",
    "plt.subplots_adjust(wspace=0.40, hspace=0.1)\n",
    "#axes.grid(False)\n",
    "f.colorbar(disp.im_, ax=axes)\n",
    "#plt.grid(False)\n",
    "#plt.rcParams['axes.grid'] = False\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KHihN9_fwgIa"
   },
   "source": [
    "###**iii. An idea about total correct and wrong predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GyRtUhiRwirZ",
    "outputId": "d78dae20-4df0-4b52-a606-9e22aef3d84a"
   },
   "outputs": [],
   "source": [
    "PN=[]\n",
    "for key, value in models.items():\n",
    "    model_data = {}\n",
    "    model_data[\"Name\"] = key\n",
    "    value.fit(X_train, y_train)\n",
    "    predicted = value.predict(X_test)\n",
    "    conf_mat = confusion_matrix(y_test, predicted)\n",
    "    model_data['True_positive'] = conf_mat[0][0]\n",
    "    model_data['False_positive'] = conf_mat[0][1]\n",
    "    model_data['False_negative'] = conf_mat[1][0]\n",
    "    model_data['True_negative']= conf_mat[1][1]\n",
    "    model_data['Correct_prediction'] = model_data['True_positive'] + model_data['True_negative']\n",
    "    model_data['Wrong_prediction'] = model_data['False_positive'] + model_data['False_negative']\n",
    "    PN.append(model_data)\n",
    "PN=pd.DataFrame(PN)\n",
    "PN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DnA6ahRLw6B9"
   },
   "source": [
    "###**iv. Evaluation Metric**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "scmqiVyZxVQx"
   },
   "outputs": [],
   "source": [
    "models = [['Adaboost', AdaBoostClassifier(random_state=42)],\n",
    "          ['Decision tree', DecisionTreeClassifier()],\n",
    "          ['KNN', KNeighborsClassifier(n_neighbors=79)],\n",
    "          ['Logistic Regression', LogisticRegression(solver='liblinear')],\n",
    "          ['Naive Bayes', GaussianNB()],\n",
    "          ['Random forest', RandomForestClassifier(n_estimators=90)],\n",
    "          ['SVM', SVC(random_state=6)]\n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E9-MOa9rxZu9"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix,roc_auc_score,recall_score,accuracy_score,precision_score\n",
    "model_1_data = []\n",
    "for name,model in models :\n",
    "    model_data = {}\n",
    "    model_data[\"Name\"] = name\n",
    "    model.fit(X_train, y_train)\n",
    "    predicted = model.predict(X_test)\n",
    "    conf_mat = confusion_matrix(y_test, predicted)\n",
    "    true_positive = conf_mat[0][0]\n",
    "    false_positive = conf_mat[0][1]\n",
    "    false_negative = conf_mat[1][0]\n",
    "    true_negative = conf_mat[1][1]\n",
    "    train_y_predicted = model.predict(X_train)\n",
    "    test_y_predicted = model.predict(X_test)\n",
    "    model_data['Train_accuracy'] = accuracy_score(y_train,train_y_predicted)\n",
    "    model_data['Test_accuracy'] = accuracy_score(y_test,test_y_predicted)\n",
    "    #model_data[\"Accuracy\"] = accuracy_score(y_test,predicted)\n",
    "    model_data['Precision'] = true_positive/(true_positive+false_positive)\n",
    "    model_data['Recall']= true_positive/(true_positive+false_negative)\n",
    "    model_data['F1_Score'] = 2*(model_data['Recall'] * model_data['Precision']) / (model_data['Recall'] + model_data['Precision'])\n",
    "    model_1_data.append(model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QABVMDMxxeAN",
    "outputId": "cfdefeb7-93a6-45ca-8c65-3868c7da45c8"
   },
   "outputs": [],
   "source": [
    "model_1_data = pd.DataFrame(model_1_data)\n",
    "# model_1_data = model_1_data.sort_values('train_accuracy',ascending=False)\n",
    "model_1_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uuVXr3pqxiGo"
   },
   "source": [
    "Here we are getting a view of all scores of evaluation metric that is train and test accuracy, Precision, Recall and F1 score.\n",
    "\n",
    "From here we can conclude that,\n",
    "\n",
    "(1) precision is always more. Its because we have very few amount of class '1' in the dependent variable. So its hard for the model to learn from the data to predict the target variable as '1'. So we try not to consider the value of precision into consideration.\n",
    "\n",
    "(2) If we have a look into f1 score, its the same in all the models except decision tree, so we can not consider this to evaluate our model performance in here.\n",
    "\n",
    "(3) So lets just consider, accuracy and recall of these models for evaluation.\n",
    "\n",
    "(4) First lets just consider recall, the more its near to 1, the more the performance of the model. We can see high recall in Adaboost, Naive Bayes and Random Forest models.\n",
    "\n",
    "(5) If we now have a look of these 3 models train and test accuracy, then Random forest is overfitting.\n",
    "\n",
    "(6) So from here, we can conclude that Naive Bayes and Adaboost is performing well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eRa9ZyMPxne_"
   },
   "source": [
    "##**3. Conclusion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XFo85nPvxjJl"
   },
   "outputs": [],
   "source": [
    "classifiers = [AdaBoostClassifier(random_state=42),DecisionTreeClassifier(),KNeighborsClassifier(n_neighbors=79),LogisticRegression(solver='liblinear'),GaussianNB(),RandomForestClassifier(n_estimators=90),SVC(random_state=6)]\n",
    "classifiers_names = ['Adaboost','Decision tree','KNN','Logistic Regression','Naive Bayes','Random Forest','SVM']\n",
    "training,testing = [],[]\n",
    "for i in classifiers:\n",
    "    i.fit(X_train, y_train)\n",
    "    train_y_predicted = i.predict(X_train)\n",
    "    test_y_predicted = i.predict(X_test)\n",
    "    tr = round(accuracy_score(y_train,train_y_predicted),4)\n",
    "    ts = round(accuracy_score(y_test,test_y_predicted),4)\n",
    "    training.append(tr)\n",
    "    testing.append(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q-nlefH6xxkR"
   },
   "outputs": [],
   "source": [
    "diff = np.array(training)-np.array(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9Ytus78cx0hu",
    "outputId": "a4784caf-23cb-45be-884c-52e7a44ce722"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,6))\n",
    "plt.plot(range(0,len(classifiers)),training,'--o',lw=2,label='Training')\n",
    "plt.plot(range(0,len(classifiers)),testing,'-o',lw=2,label='Testing')\n",
    "plt.xticks(range(0,len(classifiers)), classifiers_names, rotation=45,fontsize=14)\n",
    "plt.axvline(np.argmin(diff),linestyle=':', color='black', label=f'Best performing model')\n",
    "plt.ylabel(\"Scores\")\n",
    "plt.title(\"Comparing training and testing accuracy for our models\")\n",
    "plt.grid(True)\n",
    "plt.legend(loc='best',fontsize=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1EHB1zERx4Id"
   },
   "source": [
    "We already had concluded that adaboost and naive bayes are performing well. Lets just try all other models with hyperparamter tuning and we will try to observe the performance of these models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l-RodWPbx70c"
   },
   "source": [
    "##**Hyper-parameter tuning**\n",
    "###**Hyperparameter Tuning for all models**\n",
    "###**1. Hyper parameter tuning - KNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q9aJWDCPx5H1"
   },
   "outputs": [],
   "source": [
    "error_rate = []\n",
    "\n",
    "for i in range(1,30):\n",
    "\n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn.fit(X_train,y_train)\n",
    "    pred_i = knn.predict(X_test)\n",
    "    error_rate.append(np.mean(pred_i != y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tLMszOM0yV1B",
    "outputId": "3d902708-289d-4a53-8cab-1af19d7bbb91"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(range(1,30),error_rate,color='blue', linestyle='dashed', marker='o',\n",
    "         markerfacecolor='red', markersize=10)\n",
    "plt.title('Error Rate vs. K Value')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Error Rate');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OFedf10oyZBq",
    "outputId": "d30c6eef-a517-44c9-b208-0a97ab922831"
   },
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train,y_train)\n",
    "knn_pred = knn.predict(X_test)\n",
    "print(classification_report(y_test,knn_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ga0wq9I5ybzq",
    "outputId": "12200171-28f5-4c50-d974-cbe6660da571"
   },
   "outputs": [],
   "source": [
    "conf_mat = confusion_matrix(y_test, knn_pred)\n",
    "plt.rcParams['figure.figsize'] = (5, 5)\n",
    "sns.heatmap(conf_mat, annot = True, linewidths=.5, cmap=\"YlGnBu\")\n",
    "plt.title('Corelation Between Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JIBDVu_gyfd2"
   },
   "source": [
    "###**2. Hyper parameter tuning - Random forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YkHF7ZLxyiCS",
    "outputId": "e828c84a-05f3-4a87-be47-1a5404da4229"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid_rf = {'max_depth': [80, 90],\n",
    "    'max_features': [2, 3],\n",
    "    'min_samples_leaf': [3, 4],\n",
    "    'min_samples_split': [8, 10],\n",
    "    'n_estimators': [100, 200]}\n",
    "grid=GridSearchCV(RandomForestClassifier(),param_grid_rf,verbose=1)\n",
    "grid.fit(X_train,y_train)\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SVl6iFKJyq_B",
    "outputId": "22055014-9b04-4c96-9581-515c48e5cc5f"
   },
   "outputs": [],
   "source": [
    "grid_pred = grid.predict(X_test)\n",
    "print(classification_report(y_test,grid_pred,digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "twz50OK8yt5Q",
    "outputId": "ae917154-1d6d-4490-e51c-76fe409a95a7"
   },
   "outputs": [],
   "source": [
    "conf_mat = confusion_matrix(y_test, grid_pred)\n",
    "plt.rcParams['figure.figsize'] = (5, 5)\n",
    "sns.heatmap(conf_mat, annot = True, linewidths=.5, cmap=\"YlGnBu\")\n",
    "plt.title('Corelation Between Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IvdYXywuywwy"
   },
   "source": [
    "###**3. Hyper parameter tuning - Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lXmWlZ_uy10L",
    "outputId": "6f8a41fd-bb76-4458-f093-b97b68e8e079"
   },
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Creating the hyperparameter grid\n",
    "c_space = np.logspace(-5, 8, 15)\n",
    "param_grid_lgr = {'C': c_space}\n",
    "\n",
    "# Instantiating logistic regression classifier\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Instantiating the GridSearchCV object\n",
    "logreg_cv = GridSearchCV(logreg, param_grid_lgr, cv = 5)\n",
    "\n",
    "logreg_cv.fit(X_train, y_train)\n",
    "\n",
    "# Print the tuned parameters and score\n",
    "print(\"Tuned Logistic Regression Parameters: {}\".format(logreg_cv.best_params_))\n",
    "print(\"Best score is {}\".format(logreg_cv.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RQLO12iay5fB",
    "outputId": "d0a0e3d2-3647-4ebf-c954-14187f7f8a4f"
   },
   "outputs": [],
   "source": [
    "logreg_pred = logreg_cv.predict(X_test)\n",
    "print(classification_report(y_test,logreg_pred,digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WkTOWjPJy9AI",
    "outputId": "0073faee-d30f-40e1-e737-8e359d4c350c"
   },
   "outputs": [],
   "source": [
    "conf_mat = confusion_matrix(y_test, logreg_pred)\n",
    "plt.rcParams['figure.figsize'] = (5, 5)\n",
    "sns.heatmap(conf_mat, annot = True, linewidths=.5, cmap=\"YlGnBu\")\n",
    "plt.title('Corelation Between Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4sr4skPazQ4v"
   },
   "source": [
    "###**4. Hyper parameter tuning - Decision Tree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VsGXfzWHzWnH",
    "outputId": "0bb9ff2e-5e2f-46d2-f45c-e8d082825494"
   },
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "from scipy.stats import randint\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Creating the hyperparameter grid\n",
    "param_dist = {'max_depth': [80, 90],\n",
    "    'max_features': [2, 3],\n",
    "    'min_samples_leaf': [3, 4],\n",
    "    'min_samples_split': [8, 10],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "# Instantiating Decision Tree classifier\n",
    "tree = DecisionTreeClassifier()\n",
    "\n",
    "# Instantiating GridSearchCV object\n",
    "tree_cv = GridSearchCV(tree, param_dist, cv = 5)\n",
    "\n",
    "tree_cv.fit(X_train, y_train)\n",
    "# Print the tuned parameters and score\n",
    "print(\"Tuned Decision Tree Parameters: {}\".format(tree_cv.best_params_))\n",
    "print(\"Best score is {}\".format(tree_cv.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V0wdcE-VzaQf",
    "outputId": "433b64bd-621d-4b0d-bf6f-2b5bc3355937"
   },
   "outputs": [],
   "source": [
    "tree_pred = tree_cv.predict(X_test)\n",
    "print(classification_report(y_test,tree_pred,digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I7M4dwrRzdWA",
    "outputId": "9a7ed0c8-8a24-4924-d319-af890a948efa"
   },
   "outputs": [],
   "source": [
    "conf_mat = confusion_matrix(y_test, tree_pred)\n",
    "plt.rcParams['figure.figsize'] = (5, 5)\n",
    "sns.heatmap(conf_mat, annot = True, linewidths=.5, cmap=\"YlGnBu\")\n",
    "plt.title('Corelation Between Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gj3YNj4ZzgcQ"
   },
   "source": [
    "###**5. Hyper paramter tuning - Adaboost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sbpHScrPzi7Z",
    "outputId": "67a10b3a-0f4f-4311-aedb-77d6193360f3"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "#from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "param_grid_adb = {\n",
    "    'n_estimators': [20, 50, 70, 100],\n",
    "    'learning_rate' : [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3],\n",
    "    'n_estimators' : [100, 200, 300, 400, 500]\n",
    "             }\n",
    "\n",
    "\n",
    "#DTC = DecisionTreeClassifier(random_state = 11, max_features = \"auto\", class_weight = \"auto\",max_depth = None)\n",
    "\n",
    "adb = AdaBoostClassifier()\n",
    "\n",
    "# run grid search\n",
    "adaboost_cv = GridSearchCV(adb, param_grid=param_grid_adb, cv = 5)\n",
    "adaboost_cv.fit(X_train, y_train)\n",
    "# Print the tuned parameters and score\n",
    "print(\"Tuned Decision Tree Parameters: {}\".format(adaboost_cv.best_params_))\n",
    "print(\"Best score is {}\".format(adaboost_cv.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dHTYBPNczqK7",
    "outputId": "0f6d6e43-2440-4f04-c81c-d862f4641234"
   },
   "outputs": [],
   "source": [
    "adb_pred = adaboost_cv.predict(X_test)\n",
    "print(classification_report(y_test,adb_pred,digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vBz1c5-WztHH",
    "outputId": "061551da-c4c1-426d-f873-70d0897cd830"
   },
   "outputs": [],
   "source": [
    "conf_mat = confusion_matrix(y_test, adb_pred)\n",
    "plt.rcParams['figure.figsize'] = (5, 5)\n",
    "sns.heatmap(conf_mat, annot = True, linewidths=.5, cmap=\"YlGnBu\")\n",
    "plt.title('Corelation Between Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YcNTzH0XzwA2"
   },
   "source": [
    "###**6. Hyper parameter tuning - SVM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sZFiHq9rzyVh",
    "outputId": "8779d6ad-8fae-45d5-f727-281d64479c23"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# defining parameter range\n",
    "param_grid_svm = {'C': [0.1, 1, 10, 100, 1000],\n",
    "              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "              'kernel': ['rbf']}\n",
    "svm = SVC()\n",
    "svm_cv = GridSearchCV(svm, param_grid_svm, refit = True, verbose = 3)\n",
    "\n",
    "# fitting the model for grid search\n",
    "svm_cv.fit(X_train, y_train)\n",
    "\n",
    "# Print the tuned parameters and score\n",
    "print(\"Tuned Decision Tree Parameters: {}\".format(svm_cv.best_params_))\n",
    "print(\"Best score is {}\".format(svm_cv.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7D-bVQBvz38a",
    "outputId": "31005c4c-0355-4373-8df1-53cf610edf8e"
   },
   "outputs": [],
   "source": [
    "svm_pred = svm_cv.predict(X_test)\n",
    "print(classification_report(y_test,svm_pred,digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KQC6puQPz6oq",
    "outputId": "61ec5d4e-d733-43f9-9310-69aa10076473"
   },
   "outputs": [],
   "source": [
    "conf_mat = confusion_matrix(y_test, svm_pred)\n",
    "plt.rcParams['figure.figsize'] = (5, 5)\n",
    "sns.heatmap(conf_mat, annot = True, linewidths=.5, cmap=\"YlGnBu\")\n",
    "plt.title('Corelation Between Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8OXKw8JKz9oc"
   },
   "source": [
    "###**7. Hyper parameter tuning - Naive Bayes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9toUwBk5z_3I"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "cv_method = RepeatedStratifiedKFold(n_splits=5,\n",
    "                                    n_repeats=3,\n",
    "                                    random_state=999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qqbtJ1sJ0FzL",
    "outputId": "27183c5b-a242-4d9f-e0e9-343716d56ebe"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "params_NB = {'var_smoothing': np.logspace(0,-9, num=100)}\n",
    "\n",
    "gs_NB = GridSearchCV(estimator=nb,\n",
    "                     param_grid=params_NB,\n",
    "                     cv=cv_method,\n",
    "                     verbose=1,\n",
    "                     scoring='accuracy')\n",
    "\n",
    "Data_transformed = PowerTransformer().fit_transform(X_test)\n",
    "\n",
    "gs_NB.fit(X_test, y_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FIsBmq840IYK",
    "outputId": "c9924b8a-da83-4571-fe9c-433f1c41cf36"
   },
   "outputs": [],
   "source": [
    "gs_NB.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P-_cIN1X0K1x",
    "outputId": "5a4d2734-a6b7-42df-e584-839404145313"
   },
   "outputs": [],
   "source": [
    "gs_NB.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UCXZwsqv0PFG",
    "outputId": "d3ea201f-9beb-46ba-db12-250b8a4a50a0"
   },
   "outputs": [],
   "source": [
    "# predict the target on the test dataset\n",
    "predict_test = gs_NB.predict(Data_transformed)\n",
    "\n",
    "# Accuracy Score on test dataset\n",
    "accuracy_test = accuracy_score(y_test,predict_test)\n",
    "print('accuracy_score on test dataset : ', accuracy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9cIeXEkw0TKS"
   },
   "outputs": [],
   "source": [
    "#sns.heatmap((metrics.confusion_matrix(y_test,predict_test)),annot=True,fmt='.5g',cmap=\"YlGn\").set_title('Test Data');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a4TVBpA30VE3",
    "outputId": "44804938-b87f-4e16-97ab-9fcb6b4aba11"
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test,predict_test,digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qp82jTaZ0XX4",
    "outputId": "8604e231-07c5-4d69-ea5c-9b6e1f079424"
   },
   "outputs": [],
   "source": [
    "conf_mat = confusion_matrix(y_test, predict_test)\n",
    "plt.rcParams['figure.figsize'] = (5, 5)\n",
    "sns.heatmap(conf_mat, annot = True, linewidths=.5, cmap=\"YlGnBu\")\n",
    "plt.title('Corelation Between Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jpdtZngJ0bM-"
   },
   "source": [
    "###**( b ) Cross validation after Hyperparameter Tuning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yGCC7PeA0ti5"
   },
   "source": [
    "As we are done with the hyper parameter tuning, we now do the cross validation of these models to get a better idea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jGUbd9AQ0xGc"
   },
   "source": [
    "####**i. ROC curve after Hyper Parameter Tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WXh_2Sof046J",
    "outputId": "8b80a72c-174a-42cf-f739-89dd24b9cb85"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_roc_curve,classification_report\n",
    "\n",
    "disp = plot_roc_curve(dtc, X_test, y_test)\n",
    "plt.rcParams['figure.figsize'] = (10, 10)\n",
    "plot_roc_curve(grid,X_test, y_test, ax = disp.ax_)\n",
    "plot_roc_curve(knn,X_test, y_test, ax = disp.ax_)\n",
    "plot_roc_curve(svm_cv,X_test, y_test, ax = disp.ax_)\n",
    "plot_roc_curve(logreg_cv,X_test, y_test, ax = disp.ax_)\n",
    "plot_roc_curve(adaboost_cv,X_test, y_test, ax = disp.ax_)\n",
    "plot_roc_curve(gs_NB,X_test, y_test, ax = disp.ax_)\n",
    "plt.legend(['Random Forest','KNN','SVM','Logistic Regression','Adaboost','Naive Bayes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L4MkngNt1Kub"
   },
   "source": [
    "###**ii. Evaluation Metric after hyper parameter tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "reraoCqK1Q00"
   },
   "outputs": [],
   "source": [
    "models = [['Adaboost', AdaBoostClassifier(random_state=42)],\n",
    "          ['Adaboost after Hyperparameter Tuning',GridSearchCV(adb, param_grid=param_grid_adb, cv = 5)],\n",
    "          ['Decision Tree', DecisionTreeClassifier()],\n",
    "          ['Decision Tree after Hyperparameter Tuning',GridSearchCV(tree, param_dist, cv = 5)],\n",
    "          ['KNN', KNeighborsClassifier(n_neighbors=79)],\n",
    "          ['KNN after Hyperparameter tuning ', KNeighborsClassifier(n_neighbors=5)],\n",
    "          ['Logistic Regression', LogisticRegression(solver='liblinear')],\n",
    "          ['Logistic Regression after Hyperparameter Tuning',GridSearchCV(logreg, param_grid_lgr, cv = 5)],\n",
    "          ['Naive Bayes', GaussianNB()],\n",
    "          ['Naive Bayes after Hyperparameter tuning',\n",
    "           GridSearchCV(estimator=nb,\n",
    "                        param_grid=params_NB,\n",
    "                        cv=cv_method,\n",
    "                        verbose=1,\n",
    "                        scoring='accuracy')],\n",
    "          ['Random Forest', RandomForestClassifier(n_estimators=90)],\n",
    "          ['Random Forest after Hyperparameter Tuning',GridSearchCV(RandomForestClassifier(),param_grid_rf,verbose=1)],\n",
    "          ['SVM', SVC(random_state=6)],\n",
    "          ['SVM after Hyperparameter Tuning',GridSearchCV(svm, param_grid_svm, refit = True, verbose = 3)]\n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OUJx06A-14YC",
    "outputId": "bcca15d2-5b14-4a7a-940f-65a830c464c4"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix,roc_auc_score,recall_score,accuracy_score,precision_score\n",
    "model_1_data = []\n",
    "for name,model in models :\n",
    "    model_data = {}\n",
    "    model_data[\"Name\"] = name\n",
    "    model.fit(X_train, y_train)\n",
    "    predicted = model.predict(X_test)\n",
    "    conf_mat = confusion_matrix(y_test, predicted)\n",
    "    true_positive = conf_mat[0][0]\n",
    "    false_positive = conf_mat[0][1]\n",
    "    false_negative = conf_mat[1][0]\n",
    "    true_negative = conf_mat[1][1]\n",
    "    train_y_predicted = model.predict(X_train)\n",
    "    test_y_predicted = model.predict(X_test)\n",
    "    model_data['Train_accuracy'] = accuracy_score(y_train,train_y_predicted)\n",
    "    model_data['Test_accuracy'] = accuracy_score(y_test,test_y_predicted)\n",
    "    #model_data[\"Accuracy\"] = accuracy_score(y_test,predicted)\n",
    "    model_data['Precision'] = true_positive/(true_positive+false_positive)\n",
    "    model_data['Recall']= true_positive/(true_positive+false_negative)\n",
    "    model_data['F1_Score'] = 2*(model_data['Recall'] * model_data['Precision']) / (model_data['Recall'] + model_data['Precision'])\n",
    "    model_1_data.append(model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ean7i27m18EX"
   },
   "outputs": [],
   "source": [
    "model_2_data = pd.DataFrame(model_1_data)\n",
    "# model_1_data = model_1_data.sort_values('train_accuracy',ascending=False)\n",
    "model_2_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JwUvd2j91_gT"
   },
   "source": [
    "##**( c ) Conclusion after HyperParameter Tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DsmZfFYq2EwH",
    "outputId": "c45e9c41-9b92-41d9-95c8-2aaf9560b61f"
   },
   "outputs": [],
   "source": [
    "classifiers = [GridSearchCV(adb, param_grid=param_grid_adb, cv = 5),GridSearchCV(tree, param_dist, cv = 5),\n",
    "               KNeighborsClassifier(n_neighbors=5),GridSearchCV(logreg, param_grid_lgr, cv = 5),\n",
    "               GridSearchCV(estimator=nb,\n",
    "                        param_grid=params_NB,\n",
    "                        cv=cv_method,\n",
    "                        verbose=1,\n",
    "                        scoring='accuracy'),\n",
    "               GridSearchCV(RandomForestClassifier(),param_grid_rf,verbose=1),GridSearchCV(svm, param_grid_svm, refit = True, verbose = 3)]\n",
    "classifiers_names = ['Adaboost','Decision tree','KNN','Logistic Regression','Naive Bayes','Random Forest','SVM']\n",
    "training,testing = [],[]\n",
    "for i in classifiers:\n",
    "    i.fit(X_train, y_train)\n",
    "    train_y_predicted = i.predict(X_train)\n",
    "    test_y_predicted = i.predict(X_test)\n",
    "    tr = round(accuracy_score(y_train,train_y_predicted),4)\n",
    "    ts = round(accuracy_score(y_test,test_y_predicted),4)\n",
    "    training.append(tr)\n",
    "    testing.append(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 671
    },
    "id": "Vq5xo-ia2ILe",
    "outputId": "f764dd13-eb9b-42eb-fa4e-6a064cb3ece7"
   },
   "outputs": [],
   "source": [
    "diff = np.array(training)-np.array(testing)\n",
    "\n",
    "plt.figure(figsize=(14,6))\n",
    "plt.plot(range(0,len(classifiers)),training,'--o',lw=2,label='Training')\n",
    "plt.plot(range(0,len(classifiers)),testing,'-o',lw=2,label='Testing')\n",
    "plt.xticks(range(0,len(classifiers)), classifiers_names, rotation=45,fontsize=14)\n",
    "plt.axvline(np.argmin(diff),linestyle=':', color='black', label=f'Best performing model')\n",
    "plt.ylabel(\"Scores\")\n",
    "plt.title(\"Comparing training and testing accuracy for our models after Hyperparameter Tuning\")\n",
    "plt.grid(True)\n",
    "plt.legend(loc='best',fontsize=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mcQFXi042MHq"
   },
   "source": [
    "This is the graph after hyper parameter tuning. Even though the accuracy of some models were increased after hyper parameter tuning, still adaboost and naive bayes are the best performing models.\n",
    "\n",
    "Between those if we compare Naive bayes and Adaboost with the accuracy and recall, Naive Bayes is the best performance model."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
